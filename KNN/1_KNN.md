# ğŸ¯ K-En YakÄ±n KomÅŸu (KNN)

[Ana Sayfaya DÃ¶n](../README.md)

## ğŸ¤” Temel MantÄ±k

K-En YakÄ±n KomÅŸu (KNN), bir veri noktasÄ±nÄ± sÄ±nÄ±flandÄ±rÄ±rken, o noktaya en yakÄ±n $k$ adet komÅŸusunun etiketlerine baÅŸvuran, tembel Ã¶ÄŸrenme (lazy learning) ailesinden bir algoritmadÄ±r.

> "Bir veri noktasÄ±nÄ±n sÄ±nÄ±fÄ±, onun 'Ã§evresi' tarafÄ±ndan belirlenir."

## âš¡ Performans ve KarmaÅŸÄ±klÄ±k

| AÅŸama | KarmaÅŸÄ±klÄ±k | AÃ§Ä±klama |
| :--- | :--- | :--- |
| EÄŸitim | $O(1)$ | Veri sadece hafÄ±zada saklanÄ±r. Hesaplama yapÄ±lmaz. |
| Test (1 Ã¶rnek) | $O(n \cdot d)$ | $n$: eÄŸitim verisi sayÄ±sÄ±, $d$: Ã¶zellik boyutu. Test noktasÄ± ile $n$ adet eÄŸitim noktasÄ± arasÄ±ndaki mesafenin hesaplanmasÄ± gerekir. |

## âš ï¸ Ã–nemli Hususlar: KNN TuzaklarÄ±

### 1. Ã–lÃ§ek Problemi (Feature Scaling)

KNN, Ã–klid gibi mesafe metriklerine dayanÄ±r. EÄŸer Ã¶zellikler farklÄ± Ã¶lÃ§eklerdeyse (Ã¶rn: MaaÅŸ [10.000-100.000] ve YaÅŸ [20-60]), bÃ¼yÃ¼k Ã¶lÃ§ekli Ã¶zellik (MaaÅŸ), mesafe hesaplamalarÄ±nÄ± domine eder.

**Ã‡Ã¶zÃ¼m:** KNN kullanmadan Ã¶nce tÃ¼m Ã¶zellikler `StandardScaler` (Z-skoru) veya `MinMaxScaler` (0-1 normalizasyonu) ile **mutlaka** Ã¶lÃ§eklendirilmelidir.

### 2. YÃ¼ksek Boyut Laneti (Curse of Dimensionality)

Ã–zellik boyutu ($d$) arttÄ±kÃ§a, noktalar arasÄ±ndaki mesafeler anlamsÄ±zlaÅŸmaya baÅŸlar. YÃ¼ksek boyutlu bir uzayda, tÃ¼m noktalar birbirinden "neredeyse eÅŸit" uzaklÄ±kta gÃ¶rÃ¼nÃ¼r. Bu durum, "en yakÄ±n" komÅŸu kavramÄ±nÄ± iÅŸlevsiz hale getirir.

## ğŸ“Š SimÃ¼lasyonlar

AlgoritmanÄ±n `k` deÄŸerine ve Ã¶lÃ§eklendirmeye karÅŸÄ± hassasiyetini gÃ¶rmek iÃ§in [`notebooks/1_knn_simulations.ipynb`](../notebooks/1_knn_simulations.ipynb) defterini inceleyin.
